Sequence to Sequence Deep Learning - Sutskever, I., Vinyals, O., and Le, Q. (2014) 

Consider this example, AutoReply for emails. 
-Input: Contents of an email
-Output: Reply of Yes or No

How we can go about this: 

Preprocessing: Normalization/Tokenization
1. Feature Representation of Contents
 X:
 - 20,000 dimentional vector to represent english corpus.
 - Each index (token) represents the number of occerences of a specific word in
   the input
 - Last token is reserved for words that are out of vocabulary
 Y:
 - Coresponding y values are 1 or 0 for yes or no

Formulation: Logistic Regression Problem
 - Find W such that Wx approximates Y, since Y is either Yes or No
 - [ Probability of Yes|X, Probability of No|X ] with weights w1, w2. We want train our algorithim
   to learn this
 With SGD
 - Iterate through N number of times
 - Sample a random x in X and its corresponding y value
 - If Y == 1 (Yes), update our weights to increase Probability of Yes|X
 - If Y== 0 (No), update our weights to increase Probability of No|X
